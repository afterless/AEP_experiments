"""
filepath: /home/hice1/yhao96/AEP_experiments/OOD_evaluation.py
This script evaluates out-of-distribution (OOD) performance for responses generated by
a locally run Llama-3-8B model while applying steering via user-specified vectors.
For each combination of specified parameters, it:
  1. Loads an evaluation dataset (short, medium, long) â€“ which determines the questions.
  2. For each behavior, steering strength, and steering layer, gets the local model
     to produce long responses (the generation length is fixed via MAX_NEW_TOKENS).
  3. Sends the (question, response) pairs along with evaluation instructions (from OOD_eval_prompt)
     to a remote Groq API for evaluation.
  4. Saves the evaluation results in the folder OOD_eval_results for later plotting.
  
User-specified parameters and key variables are defined below.
"""

import os
import json
import torch
import requests
import matplotlib.pyplot as plt
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import time
from datasets import Dataset
from OOD_eval_prompt import prompt as eval_prompt_template, behavior_descriptions

# ----------------- User-Specified Parameters and Hardcoded Variables -----------------
# List of behaviors (each corresponds to a JSONL file in the evaluation directory)
BEHAVIORS = ["power-seeking-inclination"]
STEERING_STRENGTHS = [-4, -3, -2, -1, 0, 1, 2, 3, 4]
# List of dataset sizes indicating which evaluation dataset to use
PROMPT_LENS = ["medium"] # "short", "medium", "long"
# List of steering layers (which layer to apply the steering vector on)
STEERING_LAYERS = [15]
# Fixed generation length for responses
MAX_NEW_TOKENS = 160
DO_SAMPLE = False

# Model and dataset directories / endpoints
MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"
EVAL_MODEL_NAME = "meta-llama/Meta-Llama-3-70B-Instruct"
STEERING_VECTORS_DIR = "./ActAdd_sv"
RESULTS_DIR = "./OOD_eval_results"
DATA_DIR = "safety_features"
STEERED_RESPONSE_DIR = "./OOD_responses"
NUM_PROC = 10
USE_70B = False

# MODEL
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16, device_map="auto")
eval_model = AutoModelForCausalLM.from_pretrained(EVAL_MODEL_NAME, torch_dtype=torch.bfloat16, device_map="auto") if USE_70B else model
#above two use same tokenizer!!!
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"

# ----------------- End of Parameter Section -----------------

def get_hook(layer_idx, steer_vector, alpha):
    """
    Returns a hook function that at a given layer applies
    steering by adding alpha*steer_vector to the last token's hidden state.
    """
    def hook(module, inputs, output):
        v = steer_vector.to(output[0].device)
        output[0][:, -1] += alpha * v
        return output
    return hook

def add_special_tokens(item):
    prompt = item["prompt"]
    # Apply transformation to the prompt text.
    formatted_prompt = (
        f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
        f"{prompt}\n\nRespond in 70 words or less.<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
    )
    return {"formatted_prompts": formatted_prompt}

def make_prompt(item, behavior):
    description = behavior_descriptions.get(behavior, "No description available.")
    prompt = eval_prompt_template.format(
        behavior=behavior,
        description=description,
        question=item["question"],
        answer=item["answer"]
    )
    formatted_prompt = (
        f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
        f"{prompt}\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
    )
    return {"eval_prompt": formatted_prompt}


def generate_response(inputs, steering_layer, hook_fn=None):
    """
    Generates a response from the model given a question.
    If hook_fn is provided, it registers the hook at the specified steering layer.
    """
    hook_handle = None
    if hook_fn:
        hook_handle = model.model.layers[steering_layer].register_forward_hook(hook_fn)
    inputs.to(model.device)

    with torch.inference_mode():
        gen_ids = model.generate(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            max_new_tokens=MAX_NEW_TOKENS,
            use_cache=True,
            do_sample=DO_SAMPLE,
            temperature=0.6 if DO_SAMPLE else None,
            top_p=0.9 if DO_SAMPLE else None,
            pad_token_id=tokenizer.eos_token_id,
        )
    if hook_handle:
        hook_handle.remove()
    
    prompt_length = len(inputs[0])
    new_tokens = gen_ids[..., prompt_length:]
    answers = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)
    return answers

def generate_scores(prompts):
    eval_input_ids = tokenizer(
        prompts,
        return_tensors="pt",
        add_special_tokens=False,
        padding=True,
    ).to(eval_model.device)

    with torch.inference_mode():
        eval_gen_ids = eval_model.generate(
            input_ids=eval_input_ids["input_ids"],
            attention_mask=eval_input_ids["attention_mask"],
            max_new_tokens=8,
            use_cache=True,
            do_sample=False,
            temperature=None,
            top_p=None,
            pad_token_id=tokenizer.eos_token_id,
        )

    prompt_length = len(eval_input_ids[0])
    new_tokens = eval_gen_ids[..., prompt_length:]
    answers = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)
    answers = [a[3:-1].split(", ") for a in answers]
    behavior_scores = [int(a[0]) for a in answers]
    coherence_scores = [int(a[1]) for a in answers]
    return behavior_scores, coherence_scores
    

def main():
    # Iterate over each combination of behavior, dataset size, steering strength, steering layer.
    for behavior in BEHAVIORS:
        for prompt_len in PROMPT_LENS:
            ds = load_dataset("yixionghao/AEP_OOD_evaluation", data_dir=f"evaluation_data/{DATA_DIR}", data_files=f"{behavior}-{prompt_len}.json", split="train")
            processed_prompts = ds.map(add_special_tokens, remove_columns=ds.column_names, num_proc=NUM_PROC)

            input_ids = tokenizer(
                processed_prompts["formatted_prompts"],
                return_tensors="pt",
                add_special_tokens=False,
                padding=True,
            )

            behavior_scores = []
            coherency_scores = []

            for strength in STEERING_STRENGTHS:
                for steering_layer in STEERING_LAYERS:
                    print(f"\nEvaluating behavior: {behavior}, Dataset: {prompt_len}, "
                          f"Steering Strength: {strength}, Steering Layer: {steering_layer}")
                    # Load steering vector file for the behavior. We assume the file naming convention:
                    sv_file = os.path.join(STEERING_VECTORS_DIR, f"Llama-3-8B-Instruct-{behavior}.json")
                    if not os.path.exists(sv_file):
                        print(f"Steering vector file not found for {behavior}: {sv_file}")
                        continue
                    with open(sv_file, "r") as f:
                        sv_list = json.load(f)
                    # Use the steering vector from the specified layer index.
                    steering_tensor = torch.tensor(sv_list)

                    steer_vector = steering_tensor[steering_layer - 1]  # adjust for 0-indexing

                    hook_fn = get_hook(steering_layer, steer_vector, strength)
                    responses = generate_response(input_ids, steering_layer, hook_fn=hook_fn)
                    steered_qa = Dataset.from_dict({
                        "question": ds["prompt"],
                        "answer": responses
                    })

                    # Save QA pairs to a JSON file with an identifiable name.
                    # This is to manually inspect output
                    steered_qa.to_json(f"{STEERED_RESPONSE_DIR}/{behavior}-{prompt_len}-s={strength}-layer{steering_layer}.json", indent=4, lines=False)

                    behavior_accum = []
                    coherency_accum = []
                    
                    steered_qa = steered_qa.map(make_prompt, fn_kwargs={"behavior": behavior}, num_proc=NUM_PROC)
                    behavior_accum, coherency_accum = generate_scores(steered_qa["eval_prompt"])

                    behavior_scores.append(sum(behavior_accum) / len(behavior_accum))
                    coherency_scores.append(sum(coherency_accum) / len(coherency_accum))

                    # Save evaluation results for later plotting.
                    out_filename = f"{behavior}-{prompt_len}-strength-{strength}-layer-{steering_layer}.json"
                    out_filepath = os.path.join(RESULTS_DIR, out_filename)
                    with open(out_filepath, "w") as fout:
                        json.dump(behavior_results, fout, indent=4)

            print(behavior_scores)
            print(coherency_scores)

            # Plot results
            behavior_scores = [x/10 for x in behavior_scores]
            coherency_scores = [x/10 for x in coherency_scores]
            product = [x*y for x, y in zip(behavior_scores, coherency_scores)]

            plt.plot(STEERING_STRENGTHS, behavior_scores, label='behavior')
            plt.plot(STEERING_STRENGTHS, coherency_scores, label='coherence')
            plt.plot(STEERING_STRENGTHS, product, label='bxh score')

            plt.xlabel('Steering Strength')
            plt.ylabel('Scores')
            plt.title('Steering efffectiveness vs Steering Strength')
            plt.legend()
            plt.savefig('plot.png')


if __name__ == "__main__":
    main()